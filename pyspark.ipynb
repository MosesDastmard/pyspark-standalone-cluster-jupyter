{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22b706b",
   "metadata": {},
   "source": [
    "# pyspark (standalone cluster mode) is not running on jupyter!?\n",
    "\n",
    "I have seen many people have issue with pyspark while running a cluster of spark in standalone mode, but it does not work when run your application in python script or notebook.\n",
    "\n",
    "## why?\n",
    "It seems when you have a cluster up and running and try to connect your spark application written in notebook to cluster, all the resources (workers) are not available and you will get the following error:\n",
    "\n",
    "```Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources.```\n",
    "\n",
    "But if you try the same code and run line-by-line sequentionally in pyspark-shell, it works.\n",
    "\n",
    "## Solution\n",
    "\n",
    "Why not force jupyter be like pyspark-shell.\n",
    "I went through the source code of Spark till I get how ot settle the issue. You will see I tried least modification to let you know where the following originated.\n",
    "I modified ```$SPARK_HOME/python/pyspark/shell.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9d0ac",
   "metadata": {},
   "source": [
    "the following line is what I added which makes Spark works on standalone cluster in jupyter notebook.\n",
    "\n",
    "```os.environ['PYSPARK_SUBMIT_ARGS'] = '\"--master\" \"spark://192.168.1.12:7077\" \"--name\" \"PySparkShell\" \"pyspark-shell\"'```\n",
    "\n",
    "replace ```\"--master\" \"spark://192.168.1.12:7077\"``` with your spark master address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ed5710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################%%%%#########################\n",
      "/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -cp /home/moses/spark/conf/:/home/moses/spark/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --master spark://192.168.1.12:7077 --name PySparkShell pyspark-shell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/22 12:59:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.2.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.9.5 (default, Jun  4 2021 12:28:51)\n",
      "Spark context Web UI available at http://192.168.1.12:4040\n",
      "Spark context available as 'sc' (master = spark://192.168.1.12:7077, app id = app-20220222125957-0004).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "An interactive shell.\n",
    "\n",
    "This file is designed to be launched as a PYTHONSTARTUP script.\n",
    "\"\"\"\n",
    "\n",
    "import atexit\n",
    "import os\n",
    "import platform\n",
    "import warnings\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '\"--master\" \"spark://192.168.1.12:7077\" \"--name\" \"PySparkShell\" \"pyspark-shell\"'\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if os.environ.get(\"SPARK_EXECUTOR_URI\"):\n",
    "    SparkContext.setSystemProperty(\"spark.executor.uri\", os.environ[\"SPARK_EXECUTOR_URI\"])\n",
    "    print(os.environ[\"SPARK_EXECUTOR_URI\"])\n",
    "\n",
    "# print(os.environ)\n",
    "SparkContext._ensure_initialized()  # type: ignore\n",
    "try:\n",
    "    spark = SparkSession._create_shell_session()  # type: ignore\n",
    "except Exception:\n",
    "    import sys\n",
    "    import traceback\n",
    "    warnings.warn(\"Failed to initialize Spark session.\")\n",
    "    traceback.print_exc(file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sql = spark.sql\n",
    "atexit.register(lambda: sc.stop())\n",
    "\n",
    "# for compatibility\n",
    "sqlContext = spark._wrapped\n",
    "sqlCtx = sqlContext\n",
    "\n",
    "print(r\"\"\"Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version %s\n",
    "      /_/\n",
    "\"\"\" % sc.version)\n",
    "print(\"Using Python version %s (%s, %s)\" % (\n",
    "    platform.python_version(),\n",
    "    platform.python_build()[0],\n",
    "    platform.python_build()[1]))\n",
    "print(\"Spark context Web UI available at %s\" % (sc.uiWebUrl))\n",
    "print(\"Spark context available as 'sc' (master = %s, app id = %s).\" % (sc.master, sc.applicationId))\n",
    "print(\"SparkSession available as 'spark'.\")\n",
    "\n",
    "# The ./bin/pyspark script stores the old PYTHONSTARTUP value in OLD_PYTHONSTARTUP,\n",
    "# which allows us to execute the user's PYTHONSTARTUP file:\n",
    "_pythonstartup = os.environ.get('OLD_PYTHONSTARTUP')\n",
    "if _pythonstartup and os.path.isfile(_pythonstartup):\n",
    "    with open(_pythonstartup) as f:\n",
    "        code = compile(f.read(), _pythonstartup, 'exec')\n",
    "        exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15fd687f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(10)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec1706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
